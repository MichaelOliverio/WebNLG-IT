{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valutazione del dataset\n",
    "\n",
    "Verr√† eseguita la valutazione sia del dataset senza PE, sia con quello PE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_types = [\"train\", \"dev\"] \n",
    "triple_numbers = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "dataset = []\n",
    "\n",
    "sentences_en = pd.read_csv('../1_Valutazione modelli/confonto_datasets/sentences_en.csv', header=None, delimiter='aaaaaaaa', names=[0])\n",
    "\n",
    "i = 0\n",
    "\n",
    "for sentence in sentences_en[0]:\n",
    "    current_lid = \"\"\n",
    "    current_en_sentence = \"\"\n",
    "    i += 1\n",
    "    print(i)\n",
    "\n",
    "    for dataset_type in dataset_types:\n",
    "        if current_lid == \"\":\n",
    "            for triple_number in triple_numbers:\n",
    "                path = \"..\\\\webnlg\\\\it-PE\\\\\" + dataset_type + \"\\\\\" + triple_number + \"triples\"\n",
    "                file_names = []\n",
    "\n",
    "                for file_name in os.listdir(path):\n",
    "                    if os.path.isfile(os.path.join(path, file_name)):\n",
    "                        url = os.path.join(path, file_name)\n",
    "                        tree = ET.parse(url)\n",
    "                        root = tree.getroot()\n",
    "\n",
    "                        for entry in root.iter('entry'):\n",
    "                            if current_lid == \"\":\n",
    "                                for lex in entry.iter('lex'):\n",
    "                                    if lex.get('lang') == 'en':\n",
    "                                        if lex.text == sentence:\n",
    "                                            current_lid = lex.attrib['lid']\n",
    "                                            current_en_sentence = lex.text\n",
    "\n",
    "                                if current_lid != \"\":\n",
    "                                    for lex in entry.iter('lex'):\n",
    "                                        if lex.get('lang') == 'it-PE':\n",
    "                                            if lex.attrib['lid'] == current_lid:\n",
    "                                                dataset.append(str(i) + \" \" + lex.text)\n",
    "                                                print(current_en_sentence)\n",
    "                                                print(lex.text)\n",
    "                                                print('\\n')\n",
    "                                                break\n",
    "                                    \n",
    "\n",
    "dataset = np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_copy = dataset.copy()\n",
    "#rimuovi i primi 4 caratteri di ogni riga\n",
    "dataset_copy = np.char.lstrip(dataset_copy, '1234567890 ')\n",
    "\n",
    "dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_dataset = pd.DataFrame(dataset_copy)\n",
    "dt_dataset.to_csv('../1_Valutazione modelli/confonto_datasets/sentences_it_deepl_PE.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#blue \n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "#meteor\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "#pvalue\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "df_manual = pd.read_csv('../1_Valutazione modelli/confonto_datasets/sentences_it_manual.csv', header=None, delimiter='aaaaaaaa', names=[0])\n",
    "df_deepl = pd.read_csv('../1_Valutazione modelli/confonto_datasets/sentences_it_deepl.csv', header=None, delimiter='aaaaaaaa', names=[0])\n",
    "df_deepl_PE = pd.read_csv('../1_Valutazione modelli/confonto_datasets/sentences_it_deepl_PE.csv', header=None, delimiter='aaaaaaaa', names=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_evaluation(df):\n",
    "    blue_scores = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        reference = df_manual[0][i]\n",
    "        candidate = df[0][i]\n",
    "\n",
    "        #remove \" from candidate\n",
    "        candidate = candidate.replace('\"', '')\n",
    "\n",
    "        blue_score = sentence_bleu([reference], candidate, smoothing_function=SmoothingFunction().method1)\n",
    "        blue_scores.append(blue_score)\n",
    "        \n",
    "    return blue_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLUE SCORE \\n')\n",
    "\n",
    "df_deepl['bleu_score'] = blue_evaluation(df_deepl)\n",
    "print('- DeepL: ', df_deepl['bleu_score'].mean())\n",
    "\n",
    "df_deepl_PE['bleu_score'] = blue_evaluation(df_deepl_PE)\n",
    "print('- DeepL PE: ', df_deepl_PE['bleu_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### METEOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meteor_evaluation(df):\n",
    "    meteor_scores = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        reference = [word_tokenize(df_manual[0][i])]\n",
    "        candidate = word_tokenize(df[0][i])\n",
    "\n",
    "        candidate = [word.replace('\"', '') for word in candidate]\n",
    "\n",
    "        meteor_scores.append(meteor_score(reference, candidate))\n",
    "        \n",
    "    return meteor_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nMETEOR SCORE \\n')\n",
    "\n",
    "df_deepl['meteor_score'] = meteor_evaluation(df_deepl)\n",
    "print('- DeepL: ', df_deepl['meteor_score'].mean())\n",
    "\n",
    "df_deepl_PE['meteor_score'] = meteor_evaluation(df_deepl_PE)\n",
    "print('- DeepL PE: ', df_deepl_PE['meteor_score'].mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
