{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebNLG-EN Translation\n",
    "\n",
    "The translation was done using the APIs provided by DeepL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import deepl \n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "\n",
    "DEEPL_AUTH_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_lines(node):\n",
    "    for child in node.childNodes[:]:\n",
    "        if child.nodeType == child.TEXT_NODE and child.nodeValue.strip() == '':\n",
    "            node.removeChild(child)\n",
    "        elif child.nodeType == child.ELEMENT_NODE:\n",
    "            remove_empty_lines(child)\n",
    "\n",
    "def prettify(root):\n",
    "    xml_string = ET.tostring(root, encoding='utf-8')\n",
    "    dom = minidom.parseString(xml_string)\n",
    "    remove_empty_lines(dom)\n",
    "    formatted_xml = dom.toprettyxml(indent=\"  \")\n",
    "    lines = formatted_xml.split(\"\\n\") # Rimuovi la prima riga vuota\n",
    "    formatted_xml = \"\\n\".join(line for line in lines if line.strip())\n",
    "\n",
    "    return formatted_xml\n",
    "\n",
    "def save(root, url):\n",
    "    formatted_xml = prettify(root)\n",
    "    with open(url, 'w', encoding='utf-8') as f:\n",
    "        f.write(formatted_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Dev sets translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146502"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = deepl.Translator(DEEPL_AUTH_KEY) \n",
    "triple_numbers = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "dataset_types = [\"train\", \"dev\"]\n",
    "\n",
    "count = 0\n",
    "words = 0\n",
    "\n",
    "files = [\n",
    "    \"Airport.xml\", \n",
    "    \"Artist.xml\", \n",
    "    \"Astronaut.xml\", \n",
    "    \"Athlete.xml\", \n",
    "    \"Building.xml\", \n",
    "    \"CelestialBody.xml\",\n",
    "    \"City.xml\", \n",
    "    \"ComicsCharacter.xml\", \n",
    "    \"Company.xml\",\n",
    "    \"Food.xml\", \n",
    "    \"MeanOfTransportation.xml\",\n",
    "    \"Monument.xml\",\n",
    "    \"Politician.xml\",\n",
    "    \"SportsTeam.xml\", \n",
    "    \"University.xml\",\n",
    "    \"WrittenWork.xml\"\n",
    "]\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    for triple_number in triple_numbers:\n",
    "        path = \"..\\\\..\\\\..\\\\WebNLG\\\\en\\\\\" + dataset_type + \"\\\\\" + triple_number + \"triples\"\n",
    "\n",
    "        file_names = []\n",
    "        for file_name in os.listdir(path):\n",
    "            if file_name in files:\n",
    "                if os.path.isfile(os.path.join(path, file_name)):\n",
    "                    url = os.path.join(path, file_name)\n",
    "                    tree = ET.parse(url)\n",
    "                    root = tree.getroot()\n",
    "\n",
    "                    for entry in root.iter('entry'):\n",
    "                        lexs = []\n",
    "                        for lex in entry.iter('lex'):\n",
    "                            lex.set('lang', 'en')\n",
    "                            lexs.append((lex.get('comment'), lex.get('lid'), lex.text))\n",
    "                            count += 1\n",
    "                            words += len(lex.text)\n",
    "\n",
    "                        for comment, idl, text in lexs:\n",
    "                            new_lex = ET.SubElement(entry, 'lex')\n",
    "                            new_lex.set('comment', comment)\n",
    "                            new_lex.set('lid', idl)\n",
    "                            new_lex.set('lang', 'it')\n",
    "                            new_lex.text = translator.translate_text(text, target_lang=\"it\").text\n",
    "\n",
    "                    url = \"..\\\\..\\\\..\\\\WebNLG\\\\it\\\\\" + dataset_type + \"\\\\\" + triple_number + \"triples\\\\\" + file_name\n",
    "                    save(root, url)\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "949185"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#translator = deepl.Translator(DEEPL_AUTH_KEY) \n",
    "dataset_types = [\"test\"]\n",
    "\n",
    "count = 0\n",
    "words = 0\n",
    "\n",
    "files = [\n",
    "    \"rdf-to-text-generation-test-data-with-refs-en.xml\", \n",
    "    \"semantic-parsing-test-data-with-refs-en.xml\",\n",
    "]\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    path = \"..\\\\..\\\\..\\\\WebNLG\\\\en\\\\\" + dataset_type\n",
    "\n",
    "    file_names = []\n",
    "    for file_name in os.listdir(path):\n",
    "        if file_name in files:\n",
    "            if os.path.isfile(os.path.join(path, file_name)):\n",
    "                url = os.path.join(path, file_name)\n",
    "                tree = ET.parse(url)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                for entry in root.iter('entry'):\n",
    "                    lexs = []\n",
    "                    for lex in entry.iter('lex'):\n",
    "                        lex.set('lang', 'en')\n",
    "                        lexs.append((lex.get('comment'), lex.get('lid'), lex.text))\n",
    "                        count += 1\n",
    "                        words += len(lex.text)\n",
    "\n",
    "                    for comment, idl, text in lexs:\n",
    "                        new_lex = ET.SubElement(entry, 'lex')\n",
    "                        new_lex.set('comment', comment)\n",
    "                        new_lex.set('lid', idl)\n",
    "                        new_lex.set('lang', 'it')\n",
    "                        new_lex.text = translator.translate_text(text, target_lang=\"it\").text\n",
    "\n",
    "                url = \"..\\\\..\\\\..\\\\WebNLG\\\\it\\\\\" + dataset_type + \"\\\\\" + file_name\n",
    "                save(root, url)\n",
    "\n",
    "words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
