{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "import re\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# evaluation \n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in generations directory\n",
    "def open_files(directory):\n",
    "    files = os.listdir(directory)\n",
    "    files.sort()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_generations = {\n",
    "    'en': {},\n",
    "    'it': {},\n",
    "}\n",
    "\n",
    "files = open_files('generations')\n",
    "for file in files:\n",
    "    model = re.sub(r\"^fine-tuned-|-decoding-gen\\d+.csv\", \"\", file)\n",
    "    gen = re.search(r\"gen\\d+\", file).group()\n",
    "\n",
    "    model_generations = pd.read_csv('generations/' + file)\n",
    "    # convert string to list\n",
    "    model_generations['actual'] = model_generations['actual'].apply(ast.literal_eval)\n",
    "    \n",
    "    # if containt -en-\n",
    "    if '-en-' in file:\n",
    "        if model not in models_generations['en']:\n",
    "            models_generations['en'][model] = {}\n",
    "        models_generations['en'][model][gen] = model_generations\n",
    "        print(f\"model: {model} - gen: {gen}, lang: en\")\n",
    "    elif '-it-' in file:\n",
    "        if model not in models_generations['it']:\n",
    "            models_generations['it'][model] = {}\n",
    "        models_generations['it'][model][gen] = model_generations\n",
    "        print(f\"model: {model} - gen: {gen}, lang: it\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione automatica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://huggingface.co/spaces/evaluate-metric/bertscore\">Bertscore</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "bertscore_scores = {'en': {}, 'it': {}}\n",
    "\n",
    "for lang in models_generations:\n",
    "    for model in models_generations[lang]:\n",
    "        model_bertscore_scores = []\n",
    "        print(f'Language: {lang}, Model: {model}')\n",
    "        \n",
    "        gens = sorted(models_generations[lang][model])  # Ensure order (gen0, gen1, gen2)\n",
    "        for gen in tqdm(gens, desc=f'Processing {model} ({lang})'):\n",
    "            references = models_generations[lang][model][gen]['actual']\n",
    "            predictions = models_generations[lang][model][gen]['prediction']\n",
    "\n",
    "            results = bertscore.compute(predictions=predictions, references=references, lang=lang)\n",
    "            model_bertscore_scores.append(results['f1'])  # Store bertscore for each generation\n",
    "        \n",
    "        # Store the three bertscore scores instead of their mean\n",
    "        bertscore_scores[lang][model] = model_bertscore_scores\n",
    "        print(f'bertscore scores for {model} ({lang}): {model_bertscore_scores} - Average: {np.mean(model_bertscore_scores)}')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save bertscore scores\n",
    "import pickle\n",
    "\n",
    "#with open('bertscore_scores.pkl', 'wb') as f:\n",
    "#    pickle.dump(bertscore_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open bertscore_scores-exp1.pkl\n",
    "with open('bertscore_scores-exp1.pkl', 'rb') as f:\n",
    "    bertscore_scores_exp1 = pickle.load(f)\n",
    "\n",
    "# open bertscore_scores-exp2.pkl\n",
    "with open('bertscore_scores-exp3-parziale.pkl', 'rb') as f:\n",
    "    bertscore_scores_exp2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore_scores = {'it': {}}\n",
    "for lang in bertscore_scores_exp1:\n",
    "    if lang == 'it':\n",
    "        for model in bertscore_scores_exp1[lang]:\n",
    "            bertscore_scores[lang][model] = bertscore_scores_exp1[lang][model]\n",
    "            print(model)\n",
    "\n",
    "for lang in bertscore_scores_exp2:\n",
    "    if lang == 'it':\n",
    "        for model in bertscore_scores_exp2[lang]:\n",
    "            bertscore_scores[lang][model] = bertscore_scores_exp2[lang][model]\n",
    "            print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "with open('bertscore_scores-exp3.pkl', 'wb') as f:\n",
    "    pickle.dump(bertscore_scores, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bertscore_scores['it']['LLaMAntino-3-ANITA-8B-Inst-DPO-ITA-it'][0]))\n",
    "print(len(bertscore_scores['it']['LLaMAntino-3-ANITA-8B-Inst-DPO-ITA-it'][1]))\n",
    "print(len(bertscore_scores['it']['LLaMAntino-3-ANITA-8B-Inst-DPO-ITA-it'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform t-test between models using actual bertscore scores\n",
    "for lang in bertscore_scores:\n",
    "    models = list(bertscore_scores[lang].keys())\n",
    "    print(f\"\\nT-test results for {lang.upper()} models:\\n\")\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model_1, model_2 = models[i], models[j]\n",
    "            bertscore_1 = bertscore_scores[lang][model_1]  # Three bertscore scores from gen0, gen1, gen2\n",
    "            bertscore_2 = bertscore_scores[lang][model_2]  # Same for second model\n",
    "            \n",
    "            t_stat, p_value = ttest_rel(bertscore_1, bertscore_2)  # Use real bertscore values           \n",
    "            print(f\"T-test between {model_1} and {model_2}: t-stat={t_stat:.4f}, p-value={p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "\n",
    "# Perform t-test between models using the average BERTScore\n",
    "for lang in bertscore_scores:\n",
    "    models = list(bertscore_scores[lang].keys())\n",
    "    print(f\"\\nT-test results for {lang.upper()} models:\\n\")\n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        for j in range(i+1, len(models)):\n",
    "            model_1, model_2 = models[i], models[j]\n",
    "            bertscore_1 = np.mean(bertscore_scores[lang][model_1], axis=0)  # Compute average across generations\n",
    "            bertscore_2 = np.mean(bertscore_scores[lang][model_2], axis=0)  # Compute average across generations\n",
    "            \n",
    "            # Ensure both arrays have the same length\n",
    "            if bertscore_1.shape != bertscore_2.shape:\n",
    "                print(f\"Skipping T-test between {model_1} and {model_2}: Mismatched shapes {bertscore_1.shape} vs {bertscore_2.shape}\")\n",
    "                continue\n",
    "\n",
    "            # Check if the values are too similar to avoid precision loss\n",
    "            if np.allclose(bertscore_1, bertscore_2):\n",
    "                print(f\"T-test between {model_1} and {model_2}: Skipped (scores are nearly identical)\")\n",
    "                continue\n",
    "\n",
    "            # Perform the t-test\n",
    "            t_stat, p_value = ttest_rel(bertscore_1, bertscore_2)\n",
    "\n",
    "            # Print results, ensuring they are scalars\n",
    "            print(f\"T-test between {model_1} and {model_2}: t-stat={float(t_stat):.4f}, p-value={float(p_value):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "\n",
    "# Lista per salvare i risultati\n",
    "t_test_results = []\n",
    "\n",
    "# Analisi per ogni lingua\n",
    "for lang in bertscore_scores:\n",
    "    models = list(bertscore_scores[lang].keys())\n",
    "\n",
    "    # Ordina i modelli dal bertscore piÃ¹ alto al piÃ¹ basso\n",
    "    sorted_models = sorted(models, key=lambda m: np.mean(bertscore_scores[lang][m]), reverse=True)\n",
    "\n",
    "    # Creiamo una lista per la tabella finale\n",
    "    results_table = []\n",
    "\n",
    "    print(f\"\\nðŸ“Š **bertscore Score Ranking for {lang.upper()}**\\n\")\n",
    "    \n",
    "    # Confrontiamo ogni modello con quello successivo nella lista ordinata\n",
    "    for i in range(len(sorted_models)):\n",
    "        model_1 = sorted_models[i]\n",
    "        bertscore_1 = bertscore_scores[lang][model_1]  # bertscore scores per il primo modello\n",
    "        mean_bertscore_1 = np.mean(bertscore_1)  # bertscore medio del primo modello\n",
    "\n",
    "        if i < len(sorted_models) - 1:\n",
    "            model_2 = sorted_models[i + 1]\n",
    "            bertscore_2 = bertscore_scores[lang][model_2]  # bertscore scores per il secondo modello\n",
    "            mean_bertscore_2 = np.mean(bertscore_2)  # bertscore medio del secondo modello\n",
    "\n",
    "            # Calcoliamo il t-test\n",
    "            t_stat, p_value = ttest_rel(bertscore_1, bertscore_2)\n",
    "\n",
    "            # Verifica se la differenza Ã¨ statisticamente significativa\n",
    "            significant = \"âœ… Yes\" if p_value < 0.05 else \"âŒ No\"\n",
    "        else:\n",
    "            model_2, t_stat, p_value, significant = \"-\", \"-\", \"-\", \"-\"\n",
    "\n",
    "        # Aggiungiamo i risultati alla tabella\n",
    "        results_table.append({\n",
    "            \"Model\": model_1,\n",
    "            \"bertscore Score\": round(mean_bertscore_1, 4),\n",
    "            \"Compared with\": model_2,\n",
    "            \"T-Statistic\": round(t_stat, 4) if t_stat != \"-\" else \"-\",\n",
    "            \"P-Value\": round(p_value, 4) if p_value != \"-\" else \"-\",\n",
    "            \"Significant?\": significant\n",
    "        })\n",
    "\n",
    "    # Creiamo un DataFrame per la tabella finale\n",
    "    df_results = pd.DataFrame(results_table)\n",
    "\n",
    "    # Stampiamo la tabella\n",
    "    print(df_results)\n",
    "\n",
    "    # Se vuoi salvare la tabella\n",
    "    df_results.to_csv(f\"bertscore_t_test_{lang}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "\n",
    "# Lista per salvare i risultati\n",
    "t_test_results = []\n",
    "\n",
    "# Analisi per ogni lingua\n",
    "for lang in bertscore_scores:\n",
    "    models = list(bertscore_scores[lang].keys())\n",
    "\n",
    "    # Ordina i modelli dal BERTScore medio piÃ¹ alto al piÃ¹ basso\n",
    "    sorted_models = sorted(models, key=lambda m: np.mean(bertscore_scores[lang][m]), reverse=True)\n",
    "\n",
    "    # Lista per la tabella finale\n",
    "    results_table = []\n",
    "\n",
    "    print(f\"\\nðŸ“Š **BERTScore Ranking for {lang.upper()}**\\n\")\n",
    "    \n",
    "    # Confrontiamo ogni modello con quello successivo nella lista ordinata\n",
    "    for i in range(len(sorted_models) - 1):  # L'ultimo modello non ha successivo con cui confrontarsi\n",
    "        model_1 = sorted_models[i]\n",
    "        model_2 = sorted_models[i + 1]\n",
    "\n",
    "        # Prendiamo i punteggi e calcoliamo la media\n",
    "        bertscore_1 = np.array(bertscore_scores[lang][model_1]).flatten()\n",
    "        bertscore_2 = np.array(bertscore_scores[lang][model_2]).flatten()\n",
    "\n",
    "        mean_bertscore_1 = np.mean(bertscore_1)\n",
    "        mean_bertscore_2 = np.mean(bertscore_2)\n",
    "\n",
    "        # Verifica che le lunghezze siano uguali\n",
    "        if bertscore_1.shape != bertscore_2.shape:\n",
    "            print(f\"Skipping {model_1} vs {model_2}: Mismatched shapes {bertscore_1.shape} vs {bertscore_2.shape}\")\n",
    "            continue\n",
    "\n",
    "        # Se i dati sono quasi identici, evitiamo il t-test\n",
    "        if np.allclose(bertscore_1, bertscore_2):\n",
    "            print(f\"T-test skipped for {model_1} vs {model_2}: Scores are nearly identical\")\n",
    "            t_stat, p_value, significant = \"-\", \"-\", \"âŒ No (Identical Scores)\"\n",
    "        else:\n",
    "            # Calcoliamo il t-test\n",
    "            t_stat, p_value = ttest_rel(bertscore_1, bertscore_2)\n",
    "\n",
    "            # Verifica se la differenza Ã¨ statisticamente significativa\n",
    "            significant = \"âœ… Yes\" if p_value < 0.05 else \"âŒ No\"\n",
    "\n",
    "        # Aggiungiamo i risultati alla tabella\n",
    "        results_table.append({\n",
    "            \"Model\": model_1,\n",
    "            \"BERTScore\": round(mean_bertscore_1, 4),\n",
    "            \"Compared with\": model_2,\n",
    "            \"T-Statistic\": round(float(t_stat), 4) if t_stat != \"-\" else \"-\",\n",
    "            \"P-Value\": round(float(p_value), 4) if p_value != \"-\" else \"-\",\n",
    "            \"Significant?\": significant\n",
    "        })\n",
    "\n",
    "    # Crea un DataFrame per la tabella finale\n",
    "    df_results = pd.DataFrame(results_table)\n",
    "\n",
    "    # Stampiamo la tabella\n",
    "    print(df_results)\n",
    "\n",
    "    # Salviamo la tabella come CSV\n",
    "    df_results.to_csv(f\"bertscore_t_test_{lang}.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
