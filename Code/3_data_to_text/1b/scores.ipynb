{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "# evaluation \n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in generations directory\n",
    "def open_files(directory):\n",
    "    files = os.listdir(directory)\n",
    "    files.sort()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de_meta-llama2_7b_2epoch_decoding.csv',\n",
       " 'de_t5-large_2epoch_decoding.csv',\n",
       " 'en_meta-llama2_7b_2epoch_decoding.csv',\n",
       " 'en_t5-large_2epoch_decoding.csv',\n",
       " 'it_meta-llama2_7b_2epoch_decoding.csv',\n",
       " 'it_t5-large_2epoch_decoding.csv',\n",
       " 'ru_meta-llama2_7b_2epoch_decoding.csv',\n",
       " 'ru_t5-large_2epoch_decoding.csv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = {}\n",
    "\n",
    "files = open_files('generations')\n",
    "for file in files:\n",
    "    dataset = pd.read_csv('generations/' + file)\n",
    "    datasets[file] = dataset\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EN\n",
    "en_data_points = pd.read_csv('en_data_points.csv')\n",
    "en_data_points['data points'] = en_data_points['data points'].fillna('[]')\n",
    "en_data_points['data points'] = en_data_points['data points'].apply(ast.literal_eval)\n",
    "\n",
    "# IT\n",
    "it_data_points = pd.read_csv('it_data_points.csv')\n",
    "it_data_points['data points'] = it_data_points['data points'].fillna('[]')\n",
    "it_data_points['data points'] = it_data_points['data points'].apply(ast.literal_eval)\n",
    "\n",
    "# DE\n",
    "de_data_points = pd.read_csv('de_data_points.csv')\n",
    "de_data_points['data points'] = de_data_points['data points'].fillna('[]')\n",
    "de_data_points['data points'] = de_data_points['data points'].apply(ast.literal_eval)\n",
    "\n",
    "# RU\n",
    "ru_data_points = pd.read_csv('ru_data_points.csv')\n",
    "ru_data_points['data points'] = ru_data_points['data points'].fillna('[]')\n",
    "ru_data_points['data points'] = ru_data_points['data points'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incrocia i valori dei 4 data frame e mantieni solamente quelli validi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "len_dt = len(it_data_points)\n",
    "indexes = []\n",
    "for i in range(len_dt):\n",
    "    if it_data_points['data points'][i] == [] or en_data_points['data points'][i] == [] or de_data_points['data points'][i] == [] or ru_data_points['data points'][i] == []:\n",
    "        # remove rows into dataframes\n",
    "        it_data_points = it_data_points.drop(i)\n",
    "        en_data_points = en_data_points.drop(i)\n",
    "        de_data_points = de_data_points.drop(i)\n",
    "        ru_data_points = ru_data_points.drop(i)\n",
    "        indexes.append(i)\n",
    "\n",
    "# reset index\n",
    "it_data_points = it_data_points.reset_index(drop=True)\n",
    "en_data_points = en_data_points.reset_index(drop=True)\n",
    "de_data_points = de_data_points.reset_index(drop=True)\n",
    "ru_data_points = ru_data_points.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione automatica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://huggingface.co/spaces/evaluate-metric/bleu\">Bleu</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de_meta-llama2_7b_2epoch_decoding.csv BLEU: {'bleu': 0.5549915645915656, 'precisions': [0.8336517467248908, 0.6226089457788005, 0.4811492854971116, 0.3798968241173626], 'brevity_penalty': 1.0, 'length_ratio': 1.1169029111415942, 'translation_length': 14656, 'reference_length': 13122}\n",
      "de_t5-large_2epoch_decoding.csv BLEU: {'bleu': 0.5952482311928546, 'precisions': [0.868700744703926, 0.6616466630991514, 0.5211256183602303, 0.4191347897418185], 'brevity_penalty': 1.0, 'length_ratio': 1.0540313976527969, 'translation_length': 13831, 'reference_length': 13122}\n",
      "en_meta-llama2_7b_2epoch_decoding.csv BLEU: {'bleu': 0.6359652521938579, 'precisions': [0.8881840879951414, 0.7159002061269457, 0.5697875215857047, 0.45150767761954014], 'brevity_penalty': 1.0, 'length_ratio': 1.0992507974185892, 'translation_length': 14819, 'reference_length': 13481}\n",
      "en_t5-large_2epoch_decoding.csv BLEU: {'bleu': 0.6508101803483489, 'precisions': [0.9032592900145319, 0.7287789212466244, 0.5830437804030577, 0.46742070322104745], 'brevity_penalty': 1.0, 'length_ratio': 1.0719531192048068, 'translation_length': 14451, 'reference_length': 13481}\n",
      "it_meta-llama2_7b_2epoch_decoding.csv BLEU: {'bleu': 0.634933436730633, 'precisions': [0.8603348229173614, 0.7038545250298393, 0.5751871340695175, 0.4666091187318528], 'brevity_penalty': 1.0, 'length_ratio': 1.0766192733017377, 'translation_length': 14993, 'reference_length': 13926}\n",
      "it_t5-large_2epoch_decoding.csv BLEU: {'bleu': 0.6200906562437667, 'precisions': [0.8510051771666779, 0.6873893648658217, 0.5590368653256562, 0.45211122554067973], 'brevity_penalty': 1.0, 'length_ratio': 1.0680022978601178, 'translation_length': 14873, 'reference_length': 13926}\n",
      "ru_meta-llama2_7b_2epoch_decoding.csv BLEU: {'bleu': 0.06969448910382876, 'precisions': [0.2557077625570776, 0.08449792960662526, 0.04576363389092887, 0.023860705073086845], 'brevity_penalty': 1.0, 'length_ratio': 1.2350251486053956, 'translation_length': 16206, 'reference_length': 13122}\n",
      "ru_t5-large_2epoch_decoding.csv BLEU: {'bleu': 0.0679777053115093, 'precisions': [0.2712983338572776, 0.08259562385167864, 0.04579472558802566, 0.02358220355165171], 'brevity_penalty': 0.9692046697870151, 'length_ratio': 0.9696692577351014, 'translation_length': 12724, 'reference_length': 13122}\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    predictions = []\n",
    "    for index, row in datasets[dataset_name].iterrows():\n",
    "        if index not in indexes:\n",
    "            predictions.append(row['predictions'])\n",
    "\n",
    "    # check if dataset_name contains \"en\"\n",
    "    if \"en\" in dataset_name:\n",
    "        data_points = en_data_points\n",
    "    elif \"it\" in dataset_name:\n",
    "        data_points = it_data_points\n",
    "    elif \"de\" in dataset_name:\n",
    "        data_points = de_data_points\n",
    "    elif \"ru\" in dataset_name:\n",
    "        data_points = ru_data_points\n",
    "\n",
    "    references = []\n",
    "    for index, row in data_points.iterrows():\n",
    "        references.append(row['data points'])\n",
    "\n",
    "    results = bleu.compute(predictions=predictions, references=references)\n",
    "    print(dataset_name + \" BLEU: \" + str(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://huggingface.co/spaces/evaluate-metric/bertscore\">Bertscore</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de_meta-llama2_7b_2epoch_decoding.csv Bert precision: 0.9064151884714763 Bert recall: 0.9063240411281586 Bert f1: 0.9052164448102316\n",
      "de_t5-large_2epoch_decoding.csv Bert precision: 0.9161613484223684 Bert recall: 0.9096777300834655 Bert f1: 0.9118376518090566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en_meta-llama2_7b_2epoch_decoding.csv Bert precision: 0.9704108312924703 Bert recall: 0.9687355315685272 Bert f1: 0.9691636436780294\n",
      "en_t5-large_2epoch_decoding.csv Bert precision: 0.9711603197256724 Bert recall: 0.9685412779649099 Bert f1: 0.9694004866282145\n",
      "it_meta-llama2_7b_2epoch_decoding.csv Bert precision: 0.9293031032085419 Bert recall: 0.9267670160134633 Bert f1: 0.9271565098762512\n",
      "it_t5-large_2epoch_decoding.csv Bert precision: 0.9268437720139822 Bert recall: 0.9234380162556967 Bert f1: 0.9242996752262116\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    predictions = []\n",
    "    for index, row in datasets[dataset_name].iterrows():\n",
    "        if index not in indexes:\n",
    "            predictions.append(row['predictions'])\n",
    "\n",
    "    lang = \"\"\n",
    "    # check if dataset_name contains \"en\"\n",
    "    if \"en\" in dataset_name:\n",
    "        data_points = en_data_points\n",
    "        lang = \"en\"\n",
    "    elif \"it\" in dataset_name:\n",
    "        data_points = it_data_points\n",
    "        lang = \"it\"\n",
    "    elif \"de\" in dataset_name:\n",
    "        data_points = de_data_points\n",
    "        lang = \"de\"\n",
    "    elif \"ru\" in dataset_name:\n",
    "        data_points = ru_data_points\n",
    "        lang = \"ru\"\n",
    "\n",
    "    references = []\n",
    "    for index, row in data_points.iterrows():\n",
    "        references.append(row['data points'])\n",
    "\n",
    "    results = bertscore.compute(predictions=predictions, references=references, lang=lang)\n",
    "    average_precision = np.mean(results['precision'])\n",
    "    average_recall = np.mean(results['recall'])\n",
    "    average_f1 = np.mean(results['f1'])\n",
    "\n",
    "    print(dataset_name + \" Bert precision: \" + str(average_precision) + \" Bert recall: \" + str(average_recall) + \" Bert f1: \" + str(average_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://huggingface.co/spaces/evaluate-metric/chrf\">chrF++</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = load(\"chrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chrf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m data_points\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     19\u001b[0m     references\u001b[38;5;241m.\u001b[39mappend([row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata points\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m---> 21\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mchrf\u001b[49m\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpredictions, references\u001b[38;5;241m=\u001b[39mreferences, word_order\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# word order 2 per calcolare chrF++\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m chrF++: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(results))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chrf' is not defined"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    predictions = []\n",
    "    for index, row in datasets[dataset_name].iterrows():\n",
    "        if index not in indexes:\n",
    "            predictions.append(row['predictions'])\n",
    "\n",
    "    # check if dataset_name contains \"en\"\n",
    "    if \"en\" in dataset_name:\n",
    "        data_points = en_data_points\n",
    "    elif \"it\" in dataset_name:\n",
    "        data_points = it_data_points\n",
    "    elif \"de\" in dataset_name:\n",
    "        data_points = de_data_points\n",
    "    elif \"ru\" in dataset_name:\n",
    "        data_points = ru_data_points\n",
    "\n",
    "    references = []\n",
    "    for index, row in data_points.iterrows():\n",
    "        references.append([row['data points'][0]])\n",
    "\n",
    "    results = chrf.compute(predictions=predictions, references=references, word_order=2) # word order 2 per calcolare chrF++\n",
    "    print(dataset_name + \" chrF++: \" + str(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://huggingface.co/spaces/evaluate-metric/meteor\">METEOR</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteor = load('meteor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it5-large_it_2epoch_decoding.csv METEOR: {'meteor': 0.462847709014826}\n",
      "llamantino2_7b_it_2epoch_decoding.csv METEOR: {'meteor': 0.6760152549862166}\n",
      "meta-llama2_7b_it_2epoch_decoding.csv METEOR: {'meteor': 0.6661720143184371}\n",
      "minerva_3B_it_2epoch_decoding.csv METEOR: {'meteor': 0.5947401975344415}\n",
      "t5-large_it_2epoch_decoding.csv METEOR: {'meteor': 0.6432971686995566}\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    predictions = []\n",
    "    for index, row in datasets[dataset_name].iterrows():\n",
    "        if index not in indexes:\n",
    "            predictions.append(row['predictions'])\n",
    "\n",
    "    # check if dataset_name contains \"en\"\n",
    "    if \"en\" in dataset_name:\n",
    "        data_points = en_data_points\n",
    "    elif \"it\" in dataset_name:\n",
    "        data_points = it_data_points\n",
    "    elif \"de\" in dataset_name:\n",
    "        data_points = de_data_points\n",
    "    elif \"ru\" in dataset_name:\n",
    "        data_points = ru_data_points\n",
    "\n",
    "    references = []\n",
    "    for index, row in data_points.iterrows():\n",
    "        references.append(row['data points'])\n",
    "\n",
    "    results = meteor.compute(predictions=predictions, references=references)\n",
    "    print(dataset_name + \" METEOR: \" + str(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
