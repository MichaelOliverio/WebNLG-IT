{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "#bleu \n",
    "from evaluate import load\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "#ter\n",
    "from torchmetrics.text import TranslationEditRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files in generations directory\n",
    "def open_files(directory):\n",
    "    files = os.listdir(directory)\n",
    "    files.sort()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "\n",
    "files = open_files('generations')\n",
    "for file in files:\n",
    "    dataset = pd.read_csv('generations/' + file)\n",
    "    datasets[file] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione automatica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sacrebleu = load(\"sacrebleu\")\n",
    "\n",
    "def blue_evaluation(df):\n",
    "    blue_scores = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        reference = df['actuals'][i].lower()\n",
    "        candidate = df['predictions'][i].lower()\n",
    "\n",
    "        blue_score = sacrebleu.compute(predictions=[candidate], references=[reference])\n",
    "        blue_scores.append(float(blue_score['score']))\n",
    "        \n",
    "    return blue_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BLEU SCORES: \\n')\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(f'- {dataset_name}:')\n",
    "\n",
    "    datasets[dataset_name]['bleu_score'] = blue_evaluation(datasets[dataset_name])\n",
    "    print(f\"    {datasets[dataset_name]['bleu_score'].mean()}\")\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy\n",
    "dt = datasets['llamantino2_7b_it_2epoch_decoding.csv'].copy()\n",
    "#rename actuals and blue in actuals_llamantino and blue_llamantino\n",
    "dt.rename(columns={'predictions': 'predictions_llamantino', 'bleu_score': 'bleu_llamantino'}, inplace=True)\n",
    "dt['predictions_t5'] = datasets['t5-large_it_2epoch_decoding.csv']['predictions']\n",
    "dt['bleu_t5'] = datasets['t5-large_it_2epoch_decoding.csv']['bleu_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rendilo excel\n",
    "dt.to_excel('llamantino_t5.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "def bertscore_evaluation(actual, prediction, lang):\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        predictions.append(prediction.iloc[i].lower())\n",
    "        references.append(actual.iloc[i].lower())\n",
    "\n",
    "    bertscore_scores = bertscore.compute(predictions=predictions, references=references, lang=lang)\n",
    "\n",
    "    return bertscore_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('BERTSCORE SCORES: \\n')\n",
    "\n",
    "for dataset_name in datasets:\n",
    "\n",
    "    print(f'- {dataset_name} (it):')\n",
    "\n",
    "    score = bertscore_evaluation(datasets[dataset_name]['actuals'], datasets[dataset_name]['predictions'], 'it')\n",
    "    datasets[dataset_name]['bertscore_f1'] = score['f1']\n",
    "    datasets[dataset_name]['bertscore_precision'] = score['precision']\n",
    "    datasets[dataset_name]['bertscore_recall'] = score['recall']\n",
    "    print(score)\n",
    "    \n",
    "    print(f'-F1: ', np.mean(score['f1']))\n",
    "    print(f'-P: ', np.mean(score['precision']))\n",
    "    print(f'-R: ', np.mean(score['recall']))\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ter = TranslationEditRate()\n",
    "\n",
    "def ter_evaluation(actual, prediction):\n",
    "    ter_scores = []\n",
    "\n",
    "    for i in range(len(actual)):\n",
    "        reference = actual.iloc[i].lower()\n",
    "        candidate = prediction.iloc[i].lower()\n",
    "\n",
    "        ter_score = ter(candidate, [reference])\n",
    "        ter_scores.append(ter_score)\n",
    "\n",
    "    return ter_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TER SCORES: \\n')\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(f'- {dataset_name}:')\n",
    "    datasets[dataset_name]['ter'] = ter_evaluation(datasets[dataset_name]['actuals'], datasets[dataset_name]['predictions'])\n",
    "    print(f\"    {datasets[dataset_name]['ter'].mean()}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrf = load(\"chrf\")\n",
    "\n",
    "def chrf_evaluation(actual, prediction):\n",
    "    chrf_scores = []\n",
    "\n",
    "    # Utilizza tqdm per monitorare lo stato di avanzamento\n",
    "    for i in tqdm(range(len(actual)), desc=\"Calcolo CHRF\"):\n",
    "        reference = actual.iloc[i].lower()\n",
    "        candidate = prediction.iloc[i].lower()\n",
    "        chrf_score = chrf.compute(predictions=[candidate], references=[reference])['score']\n",
    "\n",
    "        chrf_scores.append(chrf_score)\n",
    "\n",
    "    return chrf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CHRF SCORES: \\n')\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(f'- {dataset_name}:')\n",
    "    datasets[dataset_name]['chrf'] = chrf_evaluation(datasets[dataset_name]['actuals'], datasets[dataset_name]['predictions'])\n",
    "    print(f\"    {datasets[dataset_name]['chrf'].mean()}\")\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
