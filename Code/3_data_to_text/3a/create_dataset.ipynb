{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costruzioni dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variabili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lingua_dataset = 'it-PE'\n",
    "tipo_dataset_generato = 'TCO'\n",
    "#random_number = str(np.random.randint(0, 100000))\n",
    "flag_shape = True\n",
    "flag_shape_type = True\n",
    "flag_size = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset per RDF2Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train e dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_types = [\"train\"] \n",
    "triple_numbers = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "max_length = 0\n",
    "for dataset_type in dataset_types:\n",
    "    for triple_number in triple_numbers:\n",
    "        path = \"webnlg\\\\it-PE\\\\\" + dataset_type + \"\\\\\" + triple_number + \"triples\"\n",
    "        file_names = []\n",
    "        for file_name in os.listdir(path):\n",
    "            if os.path.isfile(os.path.join(path, file_name)):\n",
    "                i += 1\n",
    "                url = os.path.join(path, file_name)\n",
    "                tree = ET.parse(url)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                shape = ''\n",
    "                shape_type = ''\n",
    "                size = 1\n",
    "                for entry in root.iter('entry'):\n",
    "                    if flag_shape:\n",
    "                        shape = entry.get('shape')\n",
    "\n",
    "                    if flag_shape_type:\n",
    "                        shape_type = entry.get('shape_type')\n",
    "\n",
    "                    if flag_size:\n",
    "                        size = entry.get('size')\n",
    "\n",
    "                    mts = []\n",
    "                    \n",
    "                    for modifiedtripleset in entry.iter('modifiedtripleset'):\n",
    "                        mtriples = \"\"\n",
    "\n",
    "                        if (tipo_dataset_generato == 'TC'): #senza ordinamento alfabetico\n",
    "                            for mtriple in modifiedtripleset.iter('mtriple'):                            \n",
    "                                #mtriples += mtriple.text.replace(\" | \", \" \") + \" \"\n",
    "                                mtriples += mtriple.text.replace(\" | \", \" \") + \" | \"\n",
    "\n",
    "\n",
    "                        elif (tipo_dataset_generato == 'TCO'): #con ordinamento alfabetico\n",
    "                            triples = []\n",
    "                            for mtriple in modifiedtripleset.iter('mtriple'):  \n",
    "                                triple = mtriple.text.split(\" | \")\n",
    "                                triple = (triple[0], triple[1], triple[2])\n",
    "                                triples.append(triple)\n",
    "\n",
    "                            triples.sort(key=lambda x: x[1])\n",
    "                            for triple in triples:\n",
    "                                mtriples += triple[0] + \" \" + triple[1] + \" \" + triple[2] + \" \"\n",
    "\n",
    "                        elif (tipo_dataset_generato == 'TEO'): # con ordinamento e tag\n",
    "                            triples = []\n",
    "                            for mtriple in modifiedtripleset.iter('mtriple'):  \n",
    "                                triple = mtriple.text.split(\" | \")\n",
    "                                triple = (triple[0], triple[1], triple[2])\n",
    "                                triples.append(triple)\n",
    "\n",
    "                            triples.sort(key=lambda x: x[1])\n",
    "                            for triple in triples:\n",
    "                                mtriples += \" <S> \" + triple[0] + \" <P> \" + triple[1] + \" <O> \" + triple[2]\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                        mts.append(mtriples)\n",
    "\n",
    "                    lexs = []\n",
    "                    for lex in entry.iter('lex'):\n",
    "                        if lex.get('lang') == lingua_dataset:\n",
    "                            lexs.append(lex.text)\n",
    "                            #get length of lex.text\n",
    "                            if len(lex.text) > max_length:\n",
    "                                max_length = len(lex.text)\n",
    "\n",
    "                    for mt in mts:\n",
    "                        for lex in lexs:\n",
    "                            i += 1\n",
    "                            dataset.append([mt, lex, shape, shape_type, size])\n",
    "\n",
    "dataset = np.array(dataset)\n",
    "print(max_length)\n",
    "\n",
    "dataset_pd = pd.DataFrame(dataset, columns=['triple', 'sentence', 'shape', 'shape_type', 'size'])\n",
    "dataset_pd.to_csv(random_number + \"_train_\" + tipo_dataset_generato + \"_\" + lingua_dataset + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_types = [\"dev\"] \n",
    "triple_numbers = [\"1\", \"2\", \"3\"]#, \"4\", \"5\", \"6\", \"7\"]\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for dataset_type in dataset_types:\n",
    "    for triple_number in triple_numbers:\n",
    "        path = \"webnlg\\\\it-PE\\\\\" + dataset_type + \"\\\\\" + triple_number + \"triples\"\n",
    "        file_names = []\n",
    "        for file_name in os.listdir(path):\n",
    "            if os.path.isfile(os.path.join(path, file_name)):\n",
    "                i += 1\n",
    "                url = os.path.join(path, file_name)\n",
    "                tree = ET.parse(url)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                shape = ''\n",
    "                shape_type = ''\n",
    "                size = 0\n",
    "                for entry in root.iter('entry'):\n",
    "                    if flag_shape:\n",
    "                        shape = entry.get('shape')\n",
    "\n",
    "                    if flag_shape_type:\n",
    "                        shape_type = entry.get('shape_type')\n",
    "\n",
    "                    if flag_size:\n",
    "                        size = entry.get('size')\n",
    "\n",
    "                    mts = []\n",
    "                    \n",
    "                    for modifiedtripleset in entry.iter('modifiedtripleset'):\n",
    "                        mtriples = \"\"\n",
    "                        \n",
    "                        if (tipo_dataset_generato == 'TC'): #senza ordinamento alfabetico\n",
    "                            for mtriple in modifiedtripleset.iter('mtriple'):                            \n",
    "                                mtriples += mtriple.text.replace(\" | \", \" \") + \" \"\n",
    "                        elif (tipo_dataset_generato == 'TCO'): #con ordinamento alfabetico\n",
    "                            triples = []\n",
    "                            for mtriple in modifiedtripleset.iter('mtriple'):  \n",
    "                                triple = mtriple.text.split(\" | \")\n",
    "                                triple = (triple[0], triple[1], triple[2])\n",
    "                                triples.append(triple)\n",
    "\n",
    "                            triples.sort(key=lambda x: x[1])\n",
    "                            for triple in triples:\n",
    "                                mtriples += triple[0] + \" \" + triple[1] + \" \" + triple[2] + \" \"\n",
    "                        elif (tipo_dataset_generato == 'TEO'): # con ordinamento e tag\n",
    "                            triples = []\n",
    "                            for mtriple in modifiedtripleset.iter('mtriple'):  \n",
    "                                triple = mtriple.text.split(\" | \")\n",
    "                                triple = (triple[0], triple[1], triple[2])\n",
    "                                triples.append(triple)\n",
    "\n",
    "                            triples.sort(key=lambda x: x[1])\n",
    "                            for triple in triples:\n",
    "                                mtriples += \" <S> \" + triple[0] + \" <P> \" + triple[1] + \" <O> \" + triple[2]\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "                        mts.append(mtriples)\n",
    "\n",
    "                    lexs = []\n",
    "                    for lex in entry.iter('lex'):\n",
    "                        if lex.get('lang') == lingua_dataset:\n",
    "                            lexs.append(lex.text)\n",
    "\n",
    "                    for mt in mts:\n",
    "                        for lex in lexs:\n",
    "                            i += 1\n",
    "                            dataset.append([mt, lex, shape, shape_type, size])\n",
    "\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "dataset_pd = pd.DataFrame(dataset, columns=['triple', 'sentence', 'shape', 'shape_type', 'size'])\n",
    "dataset_pd.to_csv(random_number + \"_dev_\" + tipo_dataset_generato + \"_\" + lingua_dataset + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "path = \"..\\\\..\\\\WebNLG\\\\it-PE\\\\test\"\n",
    "file_names = []\n",
    "for file_name in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, file_name)):\n",
    "        i += 1\n",
    "        url = os.path.join(path, file_name)\n",
    "        tree = ET.parse(url)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        shape = ''\n",
    "        shape_type = ''\n",
    "        size = 0\n",
    "        for entry in root.iter('entry'):\n",
    "            if flag_shape:\n",
    "                shape = entry.get('shape')\n",
    "\n",
    "            if flag_shape_type:\n",
    "                shape_type = entry.get('shape_type')\n",
    "            \n",
    "            if flag_size:\n",
    "                size = entry.get('size')\n",
    "\n",
    "            mts = []\n",
    "\n",
    "            for modifiedtripleset in entry.iter('modifiedtripleset'):\n",
    "                mtriples = \"\"\n",
    "\n",
    "                if (tipo_dataset_generato == 'TC'): #senza ordinamento alfabetico\n",
    "                    for mtriple in modifiedtripleset.iter('otriple'):                            \n",
    "                        #mtriples += mtriple.text.replace(\" | \", \" \") + \" \"\n",
    "                        mtriples += mtriple.text.replace(\" | \", \" \") + \" | \"\n",
    "                        \n",
    "                elif (tipo_dataset_generato == 'TCO'): #con ordinamento alfabetico\n",
    "                    triples = []\n",
    "\n",
    "                    for mtriple in modifiedtripleset.iter('mtriple'):\n",
    "                        triple = mtriple.text.split(\" | \")\n",
    "                        triple = (triple[0], triple[1], triple[2])\n",
    "                        triples.append(triple)\n",
    "\n",
    "                    triples.sort(key=lambda x: x[1])\n",
    "                    for triple in triples:\n",
    "                        mtriples += triple[0] + \" \" + triple[1] + \" \" + triple[2] + \" | \"\n",
    "\n",
    "\n",
    "                elif (tipo_dataset_generato == 'TEO'): # con ordinamento e tag\n",
    "                    triples = []\n",
    "                    for mtriple in modifiedtripleset.iter('mtriple'):  \n",
    "                        triple = mtriple.text.split(\" | \")\n",
    "                        triple = (triple[0], triple[1], triple[2])\n",
    "                        triples.append(triple)\n",
    "\n",
    "                    triples.sort(key=lambda x: x[1])\n",
    "                    for triple in triples:\n",
    "                        mtriples += \" <S> \" + triple[0] + \" <P> \" + triple[1] + \" <O> \" + triple[2]\n",
    "                else:\n",
    "                    break\n",
    "                            \n",
    "                mts.append(mtriples)\n",
    "\n",
    "            lexs = []\n",
    "            for lex in entry.iter('lex'):\n",
    "                if lex.get('lang') == lingua_dataset:\n",
    "                    lexs.append(lex.text)\n",
    "\n",
    "            for mt in mts:\n",
    "                for lex in lexs:\n",
    "                    i += 1\n",
    "                    dataset.append([mt, lex, shape, shape_type, size])\n",
    "\n",
    "dataset = np.array(dataset)\n",
    "\n",
    "dataset_pd = pd.DataFrame(dataset, columns=['triple', 'sentence', 'shape', 'shape_type', 'size'])\n",
    "dataset_pd.to_csv(\"pre_splitted_test_\" + tipo_dataset_generato + \"_\" + lingua_dataset + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset per EliCoDe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_numbers = [\"1\"]#, \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "dataset_types = [\"dev\"]\n",
    "\n",
    "it = True\n",
    "#it = False\n",
    "\n",
    "sentences = []\n",
    "for dataset_type in dataset_types:\n",
    "    for triple_number in triple_numbers:\n",
    "        path = \"webnlg\\\\it\\\\\" + dataset_type + \"\\\\\" + triple_number + \"triples\"\n",
    "\n",
    "        file_names = []\n",
    "        for file_name in os.listdir(path):\n",
    "            if os.path.isfile(os.path.join(path, file_name)):\n",
    "                url = os.path.join(path, file_name)\n",
    "                tree = ET.parse(url)\n",
    "                root = tree.getroot()\n",
    "\n",
    "                for entry in root.iter('entry'):\n",
    "                    originaltripleset = entry.find('modifiedtripleset')\n",
    "                    otriple = originaltripleset.find('mtriple')\n",
    "\n",
    "                    for lex in entry.iter('lex'):\n",
    "                        if lex.get('lang') == \"it\":\n",
    "                            sentences.append(lex.text)\n",
    "\n",
    "print(len(sentences))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = spacy.load(\"it_core_news_sm\")\n",
    "\n",
    "dataset = []\n",
    "for sentence in sentences:\n",
    "    sentence = tokenizer(sentence)\n",
    "\n",
    "    for token in sentence:\n",
    "        dataset.append(token.text + \" O O O\")\n",
    "    \n",
    "    dataset.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dataset into a file named test_dev_1.txt in the folder dataset-EliCoDe\n",
    "with open(\"dataset-EliCoDe\\\\test_dev_1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in dataset:\n",
    "        f.write(sentence + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione file traduzioni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_types = [\"train\", \"dev\"] \n",
    "triple_numbers = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"]\n",
    "dataset = []\n",
    "\n",
    "i = 0\n",
    "for dataset_type in dataset_types:\n",
    "    for triple_number in triple_numbers:\n",
    "        path = \"webnlg\\\\it-PE\\\\\" + dataset_type + \"\\\\\" + triple_number + \"triples\"\n",
    "        file_names = []\n",
    "        for file_name in os.listdir(path):\n",
    "            if os.path.isfile(os.path.join(path, file_name)):\n",
    "                i += 1\n",
    "                url = os.path.join(path, file_name)\n",
    "                tree = ET.parse(url)\n",
    "                root = tree.getroot()\n",
    "\n",
    "\n",
    "                for entry in root.iter('entry'):\n",
    "                    lexs = []\n",
    "                    for lex in entry.iter('lex'):\n",
    "                        if lex.get('lang') == 'it':\n",
    "                            lexs.append(lex.text)\n",
    "\n",
    "                    for lex in lexs:\n",
    "                        i += 1\n",
    "                        dataset.append(lex)\n",
    "\n",
    "dataset = np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pd = pd.DataFrame(dataset, columns=['lex'])\n",
    "dataset_pd.to_csv(\"mt.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
