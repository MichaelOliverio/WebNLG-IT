{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generazione dei dataset nelle differenti lingue\n",
    "\n",
    "In questo file vengono presi i 4 dataset nelle 4 differenti lingue e vengono estratte le triple comuni, in quanto il dataset tedesco e russo non sono allineati a quello inglese e italiano (contenogno meno triple, addirittura il tedesco non ha il test set)\n",
    "\n",
    "Successivamente, viene estratto un set di 1779 entry (in linea con la cardinalità del test set della challenge 2020), che andrà a formare il test set su cui valutare l'esperimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sentencepiece as spm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recupero dei record dei dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ge dataset caricato. Lunghezza: 7812\n",
      "en dataset caricato. Lunghezza: 16657\n",
      "it dataset caricato. Lunghezza: 16657\n",
      "ru dataset caricato. Lunghezza: 7465\n"
     ]
    }
   ],
   "source": [
    "dataset_types = [\"ge\", \"en\", \"it\", \"ru\"]\n",
    "\n",
    "# Salva i dataset nella variabile datasets\n",
    "datasets = {}\n",
    "for dataset_type in dataset_types:\n",
    "    datasets[dataset_type] = pd.read_csv(\"datasets\\\\\" + dataset_type + \"_dataset.csv\")\n",
    "    print(f'{dataset_type} dataset caricato. Lunghezza: {len(datasets[dataset_type])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizzazione delle triple\n",
    "\n",
    "Ciò viene fatto per trovare più triple comuni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per normalizzare le stringhe\n",
    "def normalize_string(s):\n",
    "    if isinstance(s, str):\n",
    "        # replace &quot; with \"\n",
    "        s = s.replace(\"&quot;\", \"\\\"\")\n",
    "        return s.lower().replace(\"’\", \"'\").replace(\"‘\", \"'\").strip()\n",
    "    return s\n",
    "\n",
    "# Normalizza le colonne \"data_unit\" in tutti i dataset\n",
    "for dataset_type in dataset_types:\n",
    "    datasets[dataset_type][\"data_unit_pe\"] = datasets[dataset_type][\"data_unit\"].apply(normalize_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>size</th>\n",
       "      <th>dataset_type</th>\n",
       "      <th>category</th>\n",
       "      <th>shape_type</th>\n",
       "      <th>data_unit</th>\n",
       "      <th>sentence</th>\n",
       "      <th>data_unit_pe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Id1</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aarhus_Airport cityServed \"Aarhus, Denmark\"</td>\n",
       "      <td>['Das Aarhus ist der Flughafen von Aarhus, Dän...</td>\n",
       "      <td>aarhus_airport cityserved \"aarhus, denmark\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Id2</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aarhus_Airport cityServed Aarhus</td>\n",
       "      <td>['Der Flughafen Aarhus dient der Stadt Aarhus. ']</td>\n",
       "      <td>aarhus_airport cityserved aarhus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Id3</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aarhus_Airport elevationAboveTheSeaLevel_(in_m...</td>\n",
       "      <td>['Der Flughafen Aarhus liegt 25 Meter über dem...</td>\n",
       "      <td>aarhus_airport elevationabovethesealevel_(in_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Id4</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aarhus_Airport location Tirstrup</td>\n",
       "      <td>['Der Flughafen Aarhus befindet sich in Tirstr...</td>\n",
       "      <td>aarhus_airport location tirstrup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Id5</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>Airport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aarhus_Airport operatingOrganisation \"Aarhus L...</td>\n",
       "      <td>['Der Flughafen Aarhus wird von Aarhus Lufthav...</td>\n",
       "      <td>aarhus_airport operatingorganisation \"aarhus l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   eid  size dataset_type category  shape_type  \\\n",
       "0  Id1     1        train  Airport         NaN   \n",
       "1  Id2     1        train  Airport         NaN   \n",
       "2  Id3     1        train  Airport         NaN   \n",
       "3  Id4     1        train  Airport         NaN   \n",
       "4  Id5     1        train  Airport         NaN   \n",
       "\n",
       "                                           data_unit  \\\n",
       "0        Aarhus_Airport cityServed \"Aarhus, Denmark\"   \n",
       "1                   Aarhus_Airport cityServed Aarhus   \n",
       "2  Aarhus_Airport elevationAboveTheSeaLevel_(in_m...   \n",
       "3                   Aarhus_Airport location Tirstrup   \n",
       "4  Aarhus_Airport operatingOrganisation \"Aarhus L...   \n",
       "\n",
       "                                            sentence  \\\n",
       "0  ['Das Aarhus ist der Flughafen von Aarhus, Dän...   \n",
       "1  ['Der Flughafen Aarhus dient der Stadt Aarhus. ']   \n",
       "2  ['Der Flughafen Aarhus liegt 25 Meter über dem...   \n",
       "3  ['Der Flughafen Aarhus befindet sich in Tirstr...   \n",
       "4  ['Der Flughafen Aarhus wird von Aarhus Lufthav...   \n",
       "\n",
       "                                        data_unit_pe  \n",
       "0        aarhus_airport cityserved \"aarhus, denmark\"  \n",
       "1                   aarhus_airport cityserved aarhus  \n",
       "2  aarhus_airport elevationabovethesealevel_(in_m...  \n",
       "3                   aarhus_airport location tirstrup  \n",
       "4  aarhus_airport operatingorganisation \"aarhus l...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"ge\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recupero delle triple comuni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di data units comuni a tutti i dataset: 3828\n"
     ]
    }
   ],
   "source": [
    "main = \"ge\"\n",
    "others = [\"it\", \"en\", \"ru\"]\n",
    "\n",
    "# Crea un set iniziale con i data_unit_pe del dataset principale\n",
    "common_data_units = set(datasets[main][\"data_unit_pe\"])\n",
    "\n",
    "# Interseca con i data_unit_pe di ciascun altro dataset\n",
    "for other in others:\n",
    "    common_data_units.intersection_update(set(datasets[other][\"data_unit_pe\"]))\n",
    "\n",
    "# Numero di data_unit comuni\n",
    "len_common = len(common_data_units)\n",
    "print(f\"Numero di data units comuni a tutti i dataset: {len_common}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united_kingdom currency pound_sterling 20_fenchurch_street location united_kingdom',\n",
       " 'john_van_den_brom club afc_ajax',\n",
       " 'ayam_penyet country java java ethnicgroup baduy singapore language english_language ayam_penyet region singapore',\n",
       " 'michele_marcolini club a.c._lumezzane',\n",
       " 'appleton_international_airport cityserved appleton,_wisconsin greenville,_wisconsin country united_states greenville,_wisconsin ispartof grand_chute,_wisconsin greenville,_wisconsin ispartof clayton,_winnebago_county,_wisconsin appleton_international_airport location greenville,_wisconsin']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_data_units = list(common_data_units)\n",
    "common_data_units[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebNLG-IT Length:  16657\n",
      "WebNLG-GE Length:  7812\n",
      "WebNLG-EN Length:  16657\n",
      "WebNLG-RU Length:  7465\n",
      "Common data units:  3828\n"
     ]
    }
   ],
   "source": [
    "print(\"WebNLG-IT Length: \", len(datasets[\"it\"]))\n",
    "print(\"WebNLG-GE Length: \", len(datasets[\"ge\"]))\n",
    "print(\"WebNLG-EN Length: \", len(datasets[\"en\"]))\n",
    "print(\"WebNLG-RU Length: \", len(datasets[\"ru\"]))\n",
    "print(\"Common data units: \", len(common_data_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione Test set e creazione train e dev con le rimanenti triple per ogni lingua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ge dataset ridotto. Lunghezza: 3871\n",
      "en dataset ridotto. Lunghezza: 3904\n",
      "it dataset ridotto. Lunghezza: 3904\n",
      "ru dataset ridotto. Lunghezza: 3845\n",
      "\n",
      "ge dataset ridotto. Lunghezza: 3828\n",
      "en dataset ridotto. Lunghezza: 3828\n",
      "it dataset ridotto. Lunghezza: 3828\n",
      "ru dataset ridotto. Lunghezza: 3828\n"
     ]
    }
   ],
   "source": [
    "# per ogni dataset, mantieni solo i record con le triple in common_data_units\n",
    "new_datasets = {}\n",
    "for dataset_type in dataset_types:\n",
    "    new_datasets[dataset_type] = datasets[dataset_type][datasets[dataset_type][\"data_unit_pe\"].isin(common_data_units)]\n",
    "    print(f'{dataset_type} dataset ridotto. Lunghezza: {len(new_datasets[dataset_type])}')\n",
    "\n",
    "print()\n",
    "\n",
    "# per ogni dataset, mantieni solo la prima occorrenza di ogni data_unit_pe\n",
    "for dataset_type in dataset_types:\n",
    "    new_datasets[dataset_type] = new_datasets[dataset_type].drop_duplicates(subset=\"data_unit_pe\")\n",
    "    print(f'{dataset_type} dataset ridotto. Lunghezza: {len(new_datasets[dataset_type])}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sincronizzazione delle verbalizzazioni: 100%|██████████| 3828/3828 [00:30<00:00, 123.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ge sincronizzato. Lunghezza: 3828\n",
      "Dataset en sincronizzato. Lunghezza: 3828\n",
      "Dataset it sincronizzato. Lunghezza: 3828\n",
      "Dataset ru sincronizzato. Lunghezza: 3828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Funzione per sincronizzare le verbalizzazioni\n",
    "def synchronize_verbalizations(new_datasets, common_data_units):\n",
    "    # Itera su ogni data_unit comune\n",
    "    for data_unit in tqdm(common_data_units, desc=\"Sincronizzazione delle verbalizzazioni\"):\n",
    "        # Ottieni le liste di verbalizzazioni per ogni dataset\n",
    "        verbalizations = {\n",
    "            \"ge\": [],\n",
    "            \"en\": [],\n",
    "            \"it\": [],\n",
    "            \"ru\": []\n",
    "        }\n",
    "        # get for each new_datasets the row with the data_unit_pe == data_unit\n",
    "        for dataset_type in dataset_types:\n",
    "            sentences = new_datasets[dataset_type][new_datasets[dataset_type][\"data_unit_pe\"] == data_unit][\"sentence\"]\n",
    "            if not sentences.empty:\n",
    "                verbalizations[dataset_type] = ast.literal_eval(sentences.values[0])\n",
    "                \n",
    "        min_len = min([len(verbalizations[dataset_type]) for dataset_type in dataset_types])\n",
    "        # controlla se le liste di verbalizzazioni sono tutte della stessa lunghezza\n",
    "        #if len(set([len(verbalizations[dataset_type]) for dataset_type in dataset_types])) != 1:\n",
    "        #    print(f\"Le liste di verbalizzazioni per la data unit {data_unit} non sono della stessa lunghezza.\")\n",
    "        #    print(\"Lunghezze:\")\n",
    "        #    for dataset_type in dataset_types:\n",
    "        #        print(f\"{dataset_type}: {len(verbalizations[dataset_type])}\")\n",
    "        #    print()\n",
    "\n",
    "        # Sincronizza le verbalizzazioni\n",
    "        for dataset_type in dataset_types:\n",
    "            # mantiene solo le prime min_len verbalizzazioni\n",
    "            verbalizations[dataset_type] = verbalizations[dataset_type][:min_len]\n",
    "\n",
    "        # Aggiorna i dataset\n",
    "        for dataset_type in dataset_types:\n",
    "            # Converti la lista troncata in stringa prima di assegnarla\n",
    "            new_datasets[dataset_type].loc[\n",
    "                new_datasets[dataset_type][\"data_unit_pe\"] == data_unit, \"sentence\"\n",
    "            ] = str(verbalizations[dataset_type])\n",
    "\n",
    "    return new_datasets\n",
    "\n",
    "# Sincronizza le verbalizzazioni\n",
    "new_datasets = synchronize_verbalizations(new_datasets, common_data_units)\n",
    "\n",
    "# Verifica dei risultati\n",
    "for dataset_type in dataset_types:\n",
    "    print(f\"Dataset {dataset_type} sincronizzato. Lunghezza: {len(new_datasets[dataset_type])}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ge: 9982\n",
      "en: 9982\n",
      "it: 9982\n",
      "ru: 9982\n"
     ]
    }
   ],
   "source": [
    "# print conta le sentences per dataset\n",
    "for dataset_type in dataset_types:\n",
    "    sentences = new_datasets[dataset_type][\"sentence\"]\n",
    "\n",
    "    i = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = ast.literal_eval(sentence)\n",
    "        i += len(sentence)\n",
    "\n",
    "    print(f\"{dataset_type}: {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order by data_unit_pe\n",
    "for dataset_type in dataset_types:\n",
    "    new_datasets[dataset_type] = new_datasets[dataset_type].sort_values(by=\"data_unit_pe\")\n",
    "\n",
    "# aggiorna gli indici di tutti i dataset\n",
    "for dataset_type in dataset_types:\n",
    "    new_datasets[dataset_type].reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ge dataset:\n",
      "Train: 3062, Dev: 383, Test: 383\n",
      "en dataset:\n",
      "Train: 3062, Dev: 383, Test: 383\n",
      "it dataset:\n",
      "Train: 3062, Dev: 383, Test: 383\n",
      "ru dataset:\n",
      "Train: 3062, Dev: 383, Test: 383\n",
      "\n",
      "ge dataset:\n",
      "Train: 7998, Dev: 991, Test: 993\n",
      "en dataset:\n",
      "Train: 7998, Dev: 991, Test: 993\n",
      "it dataset:\n",
      "Train: 7998, Dev: 991, Test: 993\n",
      "ru dataset:\n",
      "Train: 7998, Dev: 991, Test: 993\n"
     ]
    }
   ],
   "source": [
    "# per ogni dataset crea train, dev e test set\n",
    "train_ratio = 0.8\n",
    "dev_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_datasets = {}\n",
    "dev_datasets = {}\n",
    "test_datasets = {}\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    train_datasets[dataset_type] = new_datasets[dataset_type].sample(frac=train_ratio, random_state=42)\n",
    "    dev_test = new_datasets[dataset_type].drop(train_datasets[dataset_type].index)\n",
    "    dev_datasets[dataset_type] = dev_test.sample(frac=dev_ratio/(dev_ratio + test_ratio), random_state=42)\n",
    "    test_datasets[dataset_type] = dev_test.drop(dev_datasets[dataset_type].index)\n",
    "\n",
    "    print(f'{dataset_type} dataset:')\n",
    "    print(f'Train: {len(train_datasets[dataset_type])}, Dev: {len(dev_datasets[dataset_type])}, Test: {len(test_datasets[dataset_type])}')\n",
    "\n",
    "print()\n",
    "\n",
    "# train e dev hanno un array di sentence, per ognuna di esse genera un record con data_unit_pe e sentence\n",
    "train_data = {\n",
    "    \"ge\": [],\n",
    "    \"en\": [],\n",
    "    \"it\": [],\n",
    "    \"ru\": []\n",
    "}\n",
    "dev_data = {\n",
    "    \"ge\": [],\n",
    "    \"en\": [],\n",
    "    \"it\": [],\n",
    "    \"ru\": []\n",
    "}\n",
    "\n",
    "import ast\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    for i, row in train_datasets[dataset_type].iterrows():\n",
    "        array_sentences = ast.literal_eval(row[\"sentence\"])\n",
    "\n",
    "        for sentence in array_sentences:\n",
    "            train_data[dataset_type].append({\n",
    "                \"eid\": row[\"eid\"],\n",
    "                \"size\": row[\"size\"],\n",
    "                \"dataset_type\": dataset_type,\n",
    "                \"category\": row[\"category\"],\n",
    "                \"shape_type\": row[\"shape_type\"],\n",
    "                \"data_unit\": row[\"data_unit\"],\n",
    "                \"sentence\": sentence,\n",
    "            })\n",
    "\n",
    "    for i, row in dev_datasets[dataset_type].iterrows():\n",
    "        array_sentences = ast.literal_eval(row[\"sentence\"])\n",
    "\n",
    "        for sentence in array_sentences:\n",
    "            dev_data[dataset_type].append({\n",
    "                \"eid\": row[\"eid\"],\n",
    "                \"size\": row[\"size\"],\n",
    "                \"dataset_type\": dataset_type,\n",
    "                \"category\": row[\"category\"],\n",
    "                \"shape_type\": row[\"shape_type\"],\n",
    "                \"data_unit\": row[\"data_unit\"],\n",
    "                \"sentence\": sentence,\n",
    "            })\n",
    "\n",
    "for dataset_type in dataset_types:\n",
    "    train_data[dataset_type] = pd.DataFrame(train_data[dataset_type])\n",
    "    dev_data[dataset_type] = pd.DataFrame(dev_data[dataset_type])\n",
    "\n",
    "    # mischia\n",
    "    train_data[dataset_type] = train_data[dataset_type].sample(frac=1, random_state=42)\n",
    "    dev_data[dataset_type] = dev_data[dataset_type].sample(frac=1, random_state=42)\n",
    "    test_datasets[dataset_type] = test_datasets[dataset_type].sample(frac=1, random_state=42)\n",
    "\n",
    "    j = 0\n",
    "    for i, row in test_datasets[dataset_type].iterrows():\n",
    "        array_sentences = ast.literal_eval(row[\"sentence\"])\n",
    "        j += len(array_sentences)\n",
    "\n",
    "    print(f'{dataset_type} dataset:')\n",
    "    print(f'Train: {len(train_data[dataset_type])}, Dev: {len(dev_data[dataset_type])}, Test: {j}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva i dataset in formato csv\n",
    "for dataset_type in dataset_types:\n",
    "    train_data[dataset_type].to_csv(f'datasets_esperimento\\\\{dataset_type}\\\\train.csv', index=False)\n",
    "    dev_data[dataset_type].to_csv(f'datasets_esperimento\\\\{dataset_type}\\\\dev.csv', index=False)\n",
    "    test_datasets[dataset_type].to_csv(f'datasets_esperimento\\\\{dataset_type}\\\\test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
